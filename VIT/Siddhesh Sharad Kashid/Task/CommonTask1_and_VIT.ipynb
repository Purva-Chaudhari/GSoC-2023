{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f146965",
   "metadata": {},
   "source": [
    "# Common Task 1\n",
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74963717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Activation\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2f28ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\Downloads\n"
     ]
    }
   ],
   "source": [
    "%cd \"C:\\Users\\siddh\\Downloads\"\n",
    "hf = h5py.File('SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6332a722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['X', 'y']>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "558518b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(249000, 32, 32, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1 = hf.get('X')\n",
    "n1 = np.array(n1,dtype='float32')\n",
    "n1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3c1d431",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf1 = h5py.File('SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3033fafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['X', 'y']>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95acc6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(249000, 32, 32, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n2 = hf1.get('X')\n",
    "n2 = np.array(n2)\n",
    "n2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16db3417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(498000, 32, 32, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = np.row_stack((n2,n1))\n",
    "dd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a016667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(498000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = np.hstack(([0]*249000,[1]*249000))\n",
    "dl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0a3c3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5b34780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0011eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsd, dsl = unison_shuffled_copies(dd,dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ac62e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((498000, 32, 32, 2), (498000,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsd.shape , dsl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eac63170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edb4b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitted into train and validation dataset (validation data is used then for testing)\n",
    "train_matrix, test_matrix, train_labels, test_labels = train_test_split(dsd[:300000], \n",
    "                                                                        dsl[:300000],\n",
    "                                                                        test_size = 0.1,     # use 10 percent data\n",
    "                                                                        random_state = 42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77d642e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270000, 30000, 270000, 30000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_matrix), len(test_matrix), len(train_labels), len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f761a5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270000, 32, 32, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0165d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_labels.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b870f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60dd7a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(dsd[100,:,:,0],cmap=\"hot\")\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(dsd[100,:,:,1],cmap=\"hot\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f24c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ff9ca05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0003451.\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\preprocessing\\image.py:766: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (270000, 32, 32, 2) (2 channels).\n",
      "  warnings.warn(\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\preprocessing\\image.py:766: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (30000, 32, 32, 2) (2 channels).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "528/528 [==============================] - 11s 20ms/step - loss: 0.6595 - accuracy: 0.6051 - val_loss: 0.6508 - val_accuracy: 0.6173 - lr: 3.4510e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0003451.\n",
      "Epoch 2/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.6457 - accuracy: 0.6254 - val_loss: 0.6464 - val_accuracy: 0.6205 - lr: 3.4510e-04\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0003451.\n",
      "Epoch 3/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.6344 - accuracy: 0.6415 - val_loss: 0.6274 - val_accuracy: 0.6471 - lr: 3.4510e-04\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0003451.\n",
      "Epoch 4/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.6184 - accuracy: 0.6627 - val_loss: 0.6170 - val_accuracy: 0.6628 - lr: 3.4510e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0003451.\n",
      "Epoch 5/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.6057 - accuracy: 0.6767 - val_loss: 0.6002 - val_accuracy: 0.6845 - lr: 3.4510e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0003451.\n",
      "Epoch 6/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5960 - accuracy: 0.6878 - val_loss: 0.5931 - val_accuracy: 0.6900 - lr: 3.4510e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0003451.\n",
      "Epoch 7/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5899 - accuracy: 0.6938 - val_loss: 0.5892 - val_accuracy: 0.6934 - lr: 3.4510e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0003451.\n",
      "Epoch 8/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5848 - accuracy: 0.6994 - val_loss: 0.5865 - val_accuracy: 0.6971 - lr: 3.4510e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0003451.\n",
      "Epoch 9/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5823 - accuracy: 0.7016 - val_loss: 0.5807 - val_accuracy: 0.7012 - lr: 3.4510e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0003451.\n",
      "Epoch 10/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5777 - accuracy: 0.7062 - val_loss: 0.5808 - val_accuracy: 0.7006 - lr: 3.4510e-04\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.0003451.\n",
      "Epoch 11/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5751 - accuracy: 0.7079 - val_loss: 0.5802 - val_accuracy: 0.7041 - lr: 3.4510e-04\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.0003451.\n",
      "Epoch 12/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5724 - accuracy: 0.7102 - val_loss: 0.5747 - val_accuracy: 0.7083 - lr: 3.4510e-04\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.0003451.\n",
      "Epoch 13/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5705 - accuracy: 0.7109 - val_loss: 0.5753 - val_accuracy: 0.7082 - lr: 3.4510e-04\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.0003451.\n",
      "Epoch 14/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5689 - accuracy: 0.7127 - val_loss: 0.5864 - val_accuracy: 0.7002 - lr: 3.4510e-04\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.0003451.\n",
      "Epoch 15/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5668 - accuracy: 0.7147 - val_loss: 0.5689 - val_accuracy: 0.7149 - lr: 3.4510e-04\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.0003451.\n",
      "Epoch 16/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5654 - accuracy: 0.7155 - val_loss: 0.5688 - val_accuracy: 0.7115 - lr: 3.4510e-04\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.0003451.\n",
      "Epoch 17/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5641 - accuracy: 0.7162 - val_loss: 0.5665 - val_accuracy: 0.7118 - lr: 3.4510e-04\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.0003451.\n",
      "Epoch 18/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5640 - accuracy: 0.7158 - val_loss: 0.5695 - val_accuracy: 0.7101 - lr: 3.4510e-04\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.0003451.\n",
      "Epoch 19/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5615 - accuracy: 0.7184 - val_loss: 0.5662 - val_accuracy: 0.7169 - lr: 3.4510e-04\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.0003451.\n",
      "Epoch 20/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5609 - accuracy: 0.7187 - val_loss: 0.5636 - val_accuracy: 0.7133 - lr: 3.4510e-04\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.00037961.\n",
      "Epoch 21/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5603 - accuracy: 0.7192 - val_loss: 0.5634 - val_accuracy: 0.7157 - lr: 3.7961e-04\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.00037961.\n",
      "Epoch 22/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5592 - accuracy: 0.7201 - val_loss: 0.5710 - val_accuracy: 0.7082 - lr: 3.7961e-04\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.00037961.\n",
      "Epoch 23/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5578 - accuracy: 0.7212 - val_loss: 0.5611 - val_accuracy: 0.7180 - lr: 3.7961e-04\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.00037961.\n",
      "Epoch 24/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5563 - accuracy: 0.7223 - val_loss: 0.5623 - val_accuracy: 0.7173 - lr: 3.7961e-04\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.00037961.\n",
      "Epoch 25/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5563 - accuracy: 0.7224 - val_loss: 0.5646 - val_accuracy: 0.7145 - lr: 3.7961e-04\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.00037961.\n",
      "Epoch 26/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5556 - accuracy: 0.7236 - val_loss: 0.5612 - val_accuracy: 0.7177 - lr: 3.7961e-04\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.00037961.\n",
      "Epoch 27/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5544 - accuracy: 0.7235 - val_loss: 0.5602 - val_accuracy: 0.7178 - lr: 3.7961e-04\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.00037961.\n",
      "Epoch 28/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5538 - accuracy: 0.7242 - val_loss: 0.5602 - val_accuracy: 0.7175 - lr: 3.7961e-04\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.00037961.\n",
      "Epoch 29/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5528 - accuracy: 0.7251 - val_loss: 0.5639 - val_accuracy: 0.7170 - lr: 3.7961e-04\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.00037961.\n",
      "Epoch 30/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5512 - accuracy: 0.7255 - val_loss: 0.5612 - val_accuracy: 0.7166 - lr: 3.7961e-04\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.00037961.\n",
      "Epoch 31/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5516 - accuracy: 0.7257 - val_loss: 0.5589 - val_accuracy: 0.7190 - lr: 3.7961e-04\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.00037961.\n",
      "Epoch 32/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5498 - accuracy: 0.7265 - val_loss: 0.5592 - val_accuracy: 0.7181 - lr: 3.7961e-04\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 0.00037961.\n",
      "Epoch 33/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5501 - accuracy: 0.7267 - val_loss: 0.5600 - val_accuracy: 0.7184 - lr: 3.7961e-04\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 0.00037961.\n",
      "Epoch 34/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5481 - accuracy: 0.7281 - val_loss: 0.5619 - val_accuracy: 0.7152 - lr: 3.7961e-04\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 0.00037961.\n",
      "Epoch 35/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5487 - accuracy: 0.7273 - val_loss: 0.5580 - val_accuracy: 0.7207 - lr: 3.7961e-04\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 0.00037961.\n",
      "Epoch 36/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5473 - accuracy: 0.7292 - val_loss: 0.5588 - val_accuracy: 0.7201 - lr: 3.7961e-04\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 0.00037961.\n",
      "Epoch 37/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5464 - accuracy: 0.7296 - val_loss: 0.5566 - val_accuracy: 0.7218 - lr: 3.7961e-04\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 0.00037961.\n",
      "Epoch 38/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5462 - accuracy: 0.7294 - val_loss: 0.5630 - val_accuracy: 0.7162 - lr: 3.7961e-04\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 0.00037961.\n",
      "Epoch 39/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5458 - accuracy: 0.7299 - val_loss: 0.5572 - val_accuracy: 0.7209 - lr: 3.7961e-04\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 0.00037961.\n",
      "Epoch 40/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5449 - accuracy: 0.7305 - val_loss: 0.5600 - val_accuracy: 0.7186 - lr: 3.7961e-04\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 0.00041757100000000004.\n",
      "Epoch 41/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5450 - accuracy: 0.7302 - val_loss: 0.5577 - val_accuracy: 0.7202 - lr: 4.1757e-04\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 0.00041757100000000004.\n",
      "Epoch 42/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5445 - accuracy: 0.7307 - val_loss: 0.5561 - val_accuracy: 0.7219 - lr: 4.1757e-04\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 0.00041757100000000004.\n",
      "Epoch 43/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5435 - accuracy: 0.7318 - val_loss: 0.5594 - val_accuracy: 0.7201 - lr: 4.1757e-04\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 0.00041757100000000004.\n",
      "Epoch 44/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5427 - accuracy: 0.7323 - val_loss: 0.5564 - val_accuracy: 0.7220 - lr: 4.1757e-04\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 0.00041757100000000004.\n",
      "Epoch 45/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5427 - accuracy: 0.7319 - val_loss: 0.5610 - val_accuracy: 0.7176 - lr: 4.1757e-04\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 0.00041757100000000004.\n",
      "Epoch 46/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5413 - accuracy: 0.7325 - val_loss: 0.5580 - val_accuracy: 0.7195 - lr: 4.1757e-04\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 0.00041757100000000004.\n",
      "Epoch 47/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5412 - accuracy: 0.7327 - val_loss: 0.5567 - val_accuracy: 0.7210 - lr: 4.1757e-04\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 0.00041757100000000004.\n",
      "Epoch 48/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5404 - accuracy: 0.7328 - val_loss: 0.5584 - val_accuracy: 0.7217 - lr: 4.1757e-04\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 0.00041757100000000004.\n",
      "Epoch 49/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5401 - accuracy: 0.7336 - val_loss: 0.5665 - val_accuracy: 0.7129 - lr: 4.1757e-04\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 0.00041757100000000004.\n",
      "Epoch 50/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5383 - accuracy: 0.7348 - val_loss: 0.5574 - val_accuracy: 0.7214 - lr: 4.1757e-04\n",
      "\n",
      "Epoch 51: LearningRateScheduler setting learning rate to 0.00041757100000000004.\n",
      "Epoch 51/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5378 - accuracy: 0.7357 - val_loss: 0.5564 - val_accuracy: 0.7197 - lr: 4.1757e-04\n",
      "\n",
      "Epoch 52: LearningRateScheduler setting learning rate to 0.00041757100000000004.\n",
      "Epoch 52/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5380 - accuracy: 0.7352 - val_loss: 0.5605 - val_accuracy: 0.7228 - lr: 4.1757e-04\n",
      "\n",
      "Epoch 53: LearningRateScheduler setting learning rate to 0.00041757100000000004.\n",
      "Epoch 53/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5371 - accuracy: 0.7361 - val_loss: 0.5600 - val_accuracy: 0.7201 - lr: 4.1757e-04\n",
      "\n",
      "Epoch 54: LearningRateScheduler setting learning rate to 0.00041757100000000004.\n",
      "Epoch 54/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5354 - accuracy: 0.7364 - val_loss: 0.5584 - val_accuracy: 0.7196 - lr: 4.1757e-04\n",
      "\n",
      "Epoch 55: LearningRateScheduler setting learning rate to 0.00041757100000000004.\n",
      "Epoch 55/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5351 - accuracy: 0.7372 - val_loss: 0.5570 - val_accuracy: 0.7210 - lr: 4.1757e-04\n",
      "\n",
      "Epoch 56: LearningRateScheduler setting learning rate to 0.00041757100000000004.\n",
      "Epoch 56/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5339 - accuracy: 0.7378 - val_loss: 0.5591 - val_accuracy: 0.7195 - lr: 4.1757e-04\n",
      "\n",
      "Epoch 57: LearningRateScheduler setting learning rate to 0.00041757100000000004.\n",
      "Epoch 57/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5338 - accuracy: 0.7374 - val_loss: 0.5611 - val_accuracy: 0.7190 - lr: 4.1757e-04\n",
      "\n",
      "Epoch 58: LearningRateScheduler setting learning rate to 0.00041757100000000004.\n",
      "Epoch 58/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5325 - accuracy: 0.7388 - val_loss: 0.5579 - val_accuracy: 0.7207 - lr: 4.1757e-04\n",
      "\n",
      "Epoch 59: LearningRateScheduler setting learning rate to 0.00041757100000000004.\n",
      "Epoch 59/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5319 - accuracy: 0.7390 - val_loss: 0.5623 - val_accuracy: 0.7172 - lr: 4.1757e-04\n",
      "\n",
      "Epoch 60: LearningRateScheduler setting learning rate to 0.00041757100000000004.\n",
      "Epoch 60/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5313 - accuracy: 0.7387 - val_loss: 0.5587 - val_accuracy: 0.7220 - lr: 4.1757e-04\n",
      "\n",
      "Epoch 61: LearningRateScheduler setting learning rate to 0.00045932810000000015.\n",
      "Epoch 61/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5307 - accuracy: 0.7396 - val_loss: 0.5657 - val_accuracy: 0.7154 - lr: 4.5933e-04\n",
      "\n",
      "Epoch 62: LearningRateScheduler setting learning rate to 0.00045932810000000015.\n",
      "Epoch 62/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5301 - accuracy: 0.7404 - val_loss: 0.5589 - val_accuracy: 0.7207 - lr: 4.5933e-04\n",
      "\n",
      "Epoch 63: LearningRateScheduler setting learning rate to 0.00045932810000000015.\n",
      "Epoch 63/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5296 - accuracy: 0.7408 - val_loss: 0.5597 - val_accuracy: 0.7195 - lr: 4.5933e-04\n",
      "\n",
      "Epoch 64: LearningRateScheduler setting learning rate to 0.00045932810000000015.\n",
      "Epoch 64/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5287 - accuracy: 0.7412 - val_loss: 0.5659 - val_accuracy: 0.7134 - lr: 4.5933e-04\n",
      "\n",
      "Epoch 65: LearningRateScheduler setting learning rate to 0.00045932810000000015.\n",
      "Epoch 65/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5269 - accuracy: 0.7420 - val_loss: 0.5606 - val_accuracy: 0.7193 - lr: 4.5933e-04\n",
      "\n",
      "Epoch 66: LearningRateScheduler setting learning rate to 0.00045932810000000015.\n",
      "Epoch 66/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5262 - accuracy: 0.7426 - val_loss: 0.5603 - val_accuracy: 0.7191 - lr: 4.5933e-04\n",
      "\n",
      "Epoch 67: LearningRateScheduler setting learning rate to 0.00045932810000000015.\n",
      "Epoch 67/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5263 - accuracy: 0.7425 - val_loss: 0.5664 - val_accuracy: 0.7157 - lr: 4.5933e-04\n",
      "\n",
      "Epoch 68: LearningRateScheduler setting learning rate to 0.00045932810000000015.\n",
      "Epoch 68/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5249 - accuracy: 0.7431 - val_loss: 0.5650 - val_accuracy: 0.7165 - lr: 4.5933e-04\n",
      "\n",
      "Epoch 69: LearningRateScheduler setting learning rate to 0.00045932810000000015.\n",
      "Epoch 69/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5240 - accuracy: 0.7442 - val_loss: 0.5657 - val_accuracy: 0.7144 - lr: 4.5933e-04\n",
      "\n",
      "Epoch 70: LearningRateScheduler setting learning rate to 0.00045932810000000015.\n",
      "Epoch 70/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5220 - accuracy: 0.7458 - val_loss: 0.5639 - val_accuracy: 0.7153 - lr: 4.5933e-04\n",
      "\n",
      "Epoch 71: LearningRateScheduler setting learning rate to 0.00045932810000000015.\n",
      "Epoch 71/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5216 - accuracy: 0.7457 - val_loss: 0.5621 - val_accuracy: 0.7169 - lr: 4.5933e-04\n",
      "\n",
      "Epoch 72: LearningRateScheduler setting learning rate to 0.00045932810000000015.\n",
      "Epoch 72/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5206 - accuracy: 0.7469 - val_loss: 0.5643 - val_accuracy: 0.7172 - lr: 4.5933e-04\n",
      "\n",
      "Epoch 73: LearningRateScheduler setting learning rate to 0.00045932810000000015.\n",
      "Epoch 73/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5189 - accuracy: 0.7477 - val_loss: 0.5636 - val_accuracy: 0.7192 - lr: 4.5933e-04\n",
      "\n",
      "Epoch 74: LearningRateScheduler setting learning rate to 0.00045932810000000015.\n",
      "Epoch 74/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5177 - accuracy: 0.7491 - val_loss: 0.5687 - val_accuracy: 0.7116 - lr: 4.5933e-04\n",
      "\n",
      "Epoch 75: LearningRateScheduler setting learning rate to 0.00045932810000000015.\n",
      "Epoch 75/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5171 - accuracy: 0.7495 - val_loss: 0.5677 - val_accuracy: 0.7153 - lr: 4.5933e-04\n",
      "\n",
      "Epoch 76: LearningRateScheduler setting learning rate to 0.00045932810000000015.\n",
      "Epoch 76/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5160 - accuracy: 0.7498 - val_loss: 0.5708 - val_accuracy: 0.7181 - lr: 4.5933e-04\n",
      "\n",
      "Epoch 77: LearningRateScheduler setting learning rate to 0.00045932810000000015.\n",
      "Epoch 77/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5152 - accuracy: 0.7508 - val_loss: 0.5692 - val_accuracy: 0.7184 - lr: 4.5933e-04\n",
      "\n",
      "Epoch 78: LearningRateScheduler setting learning rate to 0.00045932810000000015.\n",
      "Epoch 78/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5147 - accuracy: 0.7501 - val_loss: 0.5712 - val_accuracy: 0.7170 - lr: 4.5933e-04\n",
      "\n",
      "Epoch 79: LearningRateScheduler setting learning rate to 0.00045932810000000015.\n",
      "Epoch 79/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5123 - accuracy: 0.7528 - val_loss: 0.5766 - val_accuracy: 0.7095 - lr: 4.5933e-04\n",
      "\n",
      "Epoch 80: LearningRateScheduler setting learning rate to 0.00045932810000000015.\n",
      "Epoch 80/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5116 - accuracy: 0.7527 - val_loss: 0.5690 - val_accuracy: 0.7193 - lr: 4.5933e-04\n",
      "\n",
      "Epoch 81: LearningRateScheduler setting learning rate to 0.0005052609100000002.\n",
      "Epoch 81/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5118 - accuracy: 0.7526 - val_loss: 0.5722 - val_accuracy: 0.7136 - lr: 5.0526e-04\n",
      "\n",
      "Epoch 82: LearningRateScheduler setting learning rate to 0.0005052609100000002.\n",
      "Epoch 82/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5114 - accuracy: 0.7521 - val_loss: 0.5778 - val_accuracy: 0.7135 - lr: 5.0526e-04\n",
      "\n",
      "Epoch 83: LearningRateScheduler setting learning rate to 0.0005052609100000002.\n",
      "Epoch 83/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5094 - accuracy: 0.7541 - val_loss: 0.5744 - val_accuracy: 0.7206 - lr: 5.0526e-04\n",
      "\n",
      "Epoch 84: LearningRateScheduler setting learning rate to 0.0005052609100000002.\n",
      "Epoch 84/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5086 - accuracy: 0.7547 - val_loss: 0.5755 - val_accuracy: 0.7170 - lr: 5.0526e-04\n",
      "\n",
      "Epoch 85: LearningRateScheduler setting learning rate to 0.0005052609100000002.\n",
      "Epoch 85/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5076 - accuracy: 0.7552 - val_loss: 0.5809 - val_accuracy: 0.7168 - lr: 5.0526e-04\n",
      "\n",
      "Epoch 86: LearningRateScheduler setting learning rate to 0.0005052609100000002.\n",
      "Epoch 86/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5065 - accuracy: 0.7559 - val_loss: 0.5779 - val_accuracy: 0.7126 - lr: 5.0526e-04\n",
      "\n",
      "Epoch 87: LearningRateScheduler setting learning rate to 0.0005052609100000002.\n",
      "Epoch 87/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5050 - accuracy: 0.7572 - val_loss: 0.5843 - val_accuracy: 0.7096 - lr: 5.0526e-04\n",
      "\n",
      "Epoch 88: LearningRateScheduler setting learning rate to 0.0005052609100000002.\n",
      "Epoch 88/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5038 - accuracy: 0.7570 - val_loss: 0.5836 - val_accuracy: 0.7123 - lr: 5.0526e-04\n",
      "\n",
      "Epoch 89: LearningRateScheduler setting learning rate to 0.0005052609100000002.\n",
      "Epoch 89/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5017 - accuracy: 0.7589 - val_loss: 0.5873 - val_accuracy: 0.7000 - lr: 5.0526e-04\n",
      "\n",
      "Epoch 90: LearningRateScheduler setting learning rate to 0.0005052609100000002.\n",
      "Epoch 90/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.5010 - accuracy: 0.7589 - val_loss: 0.5844 - val_accuracy: 0.7093 - lr: 5.0526e-04\n",
      "\n",
      "Epoch 91: LearningRateScheduler setting learning rate to 0.0005052609100000002.\n",
      "Epoch 91/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4993 - accuracy: 0.7605 - val_loss: 0.5848 - val_accuracy: 0.7071 - lr: 5.0526e-04\n",
      "\n",
      "Epoch 92: LearningRateScheduler setting learning rate to 0.0005052609100000002.\n",
      "Epoch 92/150\n",
      "528/528 [==============================] - 10s 20ms/step - loss: 0.4998 - accuracy: 0.7607 - val_loss: 0.5838 - val_accuracy: 0.7156 - lr: 5.0526e-04\n",
      "\n",
      "Epoch 93: LearningRateScheduler setting learning rate to 0.0005052609100000002.\n",
      "Epoch 93/150\n",
      "528/528 [==============================] - 10s 20ms/step - loss: 0.4968 - accuracy: 0.7629 - val_loss: 0.5975 - val_accuracy: 0.7094 - lr: 5.0526e-04\n",
      "\n",
      "Epoch 94: LearningRateScheduler setting learning rate to 0.0005052609100000002.\n",
      "Epoch 94/150\n",
      "528/528 [==============================] - 10s 20ms/step - loss: 0.4951 - accuracy: 0.7631 - val_loss: 0.5866 - val_accuracy: 0.7112 - lr: 5.0526e-04\n",
      "\n",
      "Epoch 95: LearningRateScheduler setting learning rate to 0.0005052609100000002.\n",
      "Epoch 95/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4948 - accuracy: 0.7631 - val_loss: 0.5902 - val_accuracy: 0.7067 - lr: 5.0526e-04\n",
      "\n",
      "Epoch 96: LearningRateScheduler setting learning rate to 0.0005052609100000002.\n",
      "Epoch 96/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4920 - accuracy: 0.7652 - val_loss: 0.5994 - val_accuracy: 0.7060 - lr: 5.0526e-04\n",
      "\n",
      "Epoch 97: LearningRateScheduler setting learning rate to 0.0005052609100000002.\n",
      "Epoch 97/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4923 - accuracy: 0.7648 - val_loss: 0.5965 - val_accuracy: 0.7137 - lr: 5.0526e-04\n",
      "\n",
      "Epoch 98: LearningRateScheduler setting learning rate to 0.0005052609100000002.\n",
      "Epoch 98/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4905 - accuracy: 0.7654 - val_loss: 0.5987 - val_accuracy: 0.7046 - lr: 5.0526e-04\n",
      "\n",
      "Epoch 99: LearningRateScheduler setting learning rate to 0.0005052609100000002.\n",
      "Epoch 99/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4886 - accuracy: 0.7667 - val_loss: 0.5936 - val_accuracy: 0.7034 - lr: 5.0526e-04\n",
      "\n",
      "Epoch 100: LearningRateScheduler setting learning rate to 0.0005052609100000002.\n",
      "Epoch 100/150\n",
      "528/528 [==============================] - 10s 20ms/step - loss: 0.4876 - accuracy: 0.7675 - val_loss: 0.6015 - val_accuracy: 0.7109 - lr: 5.0526e-04\n",
      "\n",
      "Epoch 101: LearningRateScheduler setting learning rate to 0.0005557870010000002.\n",
      "Epoch 101/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4889 - accuracy: 0.7672 - val_loss: 0.6002 - val_accuracy: 0.7019 - lr: 5.5579e-04\n",
      "\n",
      "Epoch 102: LearningRateScheduler setting learning rate to 0.0005557870010000002.\n",
      "Epoch 102/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4872 - accuracy: 0.7680 - val_loss: 0.5985 - val_accuracy: 0.7007 - lr: 5.5579e-04\n",
      "\n",
      "Epoch 103: LearningRateScheduler setting learning rate to 0.0005557870010000002.\n",
      "Epoch 103/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4856 - accuracy: 0.7689 - val_loss: 0.5959 - val_accuracy: 0.7075 - lr: 5.5579e-04\n",
      "\n",
      "Epoch 104: LearningRateScheduler setting learning rate to 0.0005557870010000002.\n",
      "Epoch 104/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4843 - accuracy: 0.7696 - val_loss: 0.6024 - val_accuracy: 0.7048 - lr: 5.5579e-04\n",
      "\n",
      "Epoch 105: LearningRateScheduler setting learning rate to 0.0005557870010000002.\n",
      "Epoch 105/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4829 - accuracy: 0.7707 - val_loss: 0.6040 - val_accuracy: 0.7028 - lr: 5.5579e-04\n",
      "\n",
      "Epoch 106: LearningRateScheduler setting learning rate to 0.0005557870010000002.\n",
      "Epoch 106/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4821 - accuracy: 0.7710 - val_loss: 0.6046 - val_accuracy: 0.7083 - lr: 5.5579e-04\n",
      "\n",
      "Epoch 107: LearningRateScheduler setting learning rate to 0.0005557870010000002.\n",
      "Epoch 107/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4795 - accuracy: 0.7719 - val_loss: 0.6084 - val_accuracy: 0.7002 - lr: 5.5579e-04\n",
      "\n",
      "Epoch 108: LearningRateScheduler setting learning rate to 0.0005557870010000002.\n",
      "Epoch 108/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4788 - accuracy: 0.7725 - val_loss: 0.6132 - val_accuracy: 0.6977 - lr: 5.5579e-04\n",
      "\n",
      "Epoch 109: LearningRateScheduler setting learning rate to 0.0005557870010000002.\n",
      "Epoch 109/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4776 - accuracy: 0.7741 - val_loss: 0.6126 - val_accuracy: 0.6994 - lr: 5.5579e-04\n",
      "\n",
      "Epoch 110: LearningRateScheduler setting learning rate to 0.0005557870010000002.\n",
      "Epoch 110/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4756 - accuracy: 0.7750 - val_loss: 0.6181 - val_accuracy: 0.6972 - lr: 5.5579e-04\n",
      "\n",
      "Epoch 111: LearningRateScheduler setting learning rate to 0.0005557870010000002.\n",
      "Epoch 111/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4735 - accuracy: 0.7760 - val_loss: 0.6223 - val_accuracy: 0.7054 - lr: 5.5579e-04\n",
      "\n",
      "Epoch 112: LearningRateScheduler setting learning rate to 0.0005557870010000002.\n",
      "Epoch 112/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4729 - accuracy: 0.7768 - val_loss: 0.6119 - val_accuracy: 0.7035 - lr: 5.5579e-04\n",
      "\n",
      "Epoch 113: LearningRateScheduler setting learning rate to 0.0005557870010000002.\n",
      "Epoch 113/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4725 - accuracy: 0.7770 - val_loss: 0.6237 - val_accuracy: 0.6986 - lr: 5.5579e-04\n",
      "\n",
      "Epoch 114: LearningRateScheduler setting learning rate to 0.0005557870010000002.\n",
      "Epoch 114/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4701 - accuracy: 0.7782 - val_loss: 0.6187 - val_accuracy: 0.6970 - lr: 5.5579e-04\n",
      "\n",
      "Epoch 115: LearningRateScheduler setting learning rate to 0.0005557870010000002.\n",
      "Epoch 115/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4672 - accuracy: 0.7795 - val_loss: 0.6288 - val_accuracy: 0.6993 - lr: 5.5579e-04\n",
      "\n",
      "Epoch 116: LearningRateScheduler setting learning rate to 0.0005557870010000002.\n",
      "Epoch 116/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4666 - accuracy: 0.7798 - val_loss: 0.6244 - val_accuracy: 0.7003 - lr: 5.5579e-04\n",
      "\n",
      "Epoch 117: LearningRateScheduler setting learning rate to 0.0005557870010000002.\n",
      "Epoch 117/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4650 - accuracy: 0.7811 - val_loss: 0.6261 - val_accuracy: 0.7005 - lr: 5.5579e-04\n",
      "\n",
      "Epoch 118: LearningRateScheduler setting learning rate to 0.0005557870010000002.\n",
      "Epoch 118/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4643 - accuracy: 0.7816 - val_loss: 0.6301 - val_accuracy: 0.7001 - lr: 5.5579e-04\n",
      "\n",
      "Epoch 119: LearningRateScheduler setting learning rate to 0.0005557870010000002.\n",
      "Epoch 119/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4618 - accuracy: 0.7827 - val_loss: 0.6324 - val_accuracy: 0.7055 - lr: 5.5579e-04\n",
      "\n",
      "Epoch 120: LearningRateScheduler setting learning rate to 0.0005557870010000002.\n",
      "Epoch 120/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4618 - accuracy: 0.7828 - val_loss: 0.6353 - val_accuracy: 0.7028 - lr: 5.5579e-04\n",
      "\n",
      "Epoch 121: LearningRateScheduler setting learning rate to 0.0006113657011000002.\n",
      "Epoch 121/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4623 - accuracy: 0.7824 - val_loss: 0.6353 - val_accuracy: 0.6922 - lr: 6.1137e-04\n",
      "\n",
      "Epoch 122: LearningRateScheduler setting learning rate to 0.0006113657011000002.\n",
      "Epoch 122/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4612 - accuracy: 0.7832 - val_loss: 0.6348 - val_accuracy: 0.6972 - lr: 6.1137e-04\n",
      "\n",
      "Epoch 123: LearningRateScheduler setting learning rate to 0.0006113657011000002.\n",
      "Epoch 123/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4613 - accuracy: 0.7828 - val_loss: 0.6492 - val_accuracy: 0.6929 - lr: 6.1137e-04\n",
      "\n",
      "Epoch 124: LearningRateScheduler setting learning rate to 0.0006113657011000002.\n",
      "Epoch 124/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4584 - accuracy: 0.7851 - val_loss: 0.6380 - val_accuracy: 0.6881 - lr: 6.1137e-04\n",
      "\n",
      "Epoch 125: LearningRateScheduler setting learning rate to 0.0006113657011000002.\n",
      "Epoch 125/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4589 - accuracy: 0.7847 - val_loss: 0.6423 - val_accuracy: 0.7004 - lr: 6.1137e-04\n",
      "\n",
      "Epoch 126: LearningRateScheduler setting learning rate to 0.0006113657011000002.\n",
      "Epoch 126/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4562 - accuracy: 0.7860 - val_loss: 0.6429 - val_accuracy: 0.6910 - lr: 6.1137e-04\n",
      "\n",
      "Epoch 127: LearningRateScheduler setting learning rate to 0.0006113657011000002.\n",
      "Epoch 127/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4548 - accuracy: 0.7865 - val_loss: 0.6381 - val_accuracy: 0.6959 - lr: 6.1137e-04\n",
      "\n",
      "Epoch 128: LearningRateScheduler setting learning rate to 0.0006113657011000002.\n",
      "Epoch 128/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4539 - accuracy: 0.7872 - val_loss: 0.6392 - val_accuracy: 0.7019 - lr: 6.1137e-04\n",
      "\n",
      "Epoch 129: LearningRateScheduler setting learning rate to 0.0006113657011000002.\n",
      "Epoch 129/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4523 - accuracy: 0.7881 - val_loss: 0.6599 - val_accuracy: 0.6857 - lr: 6.1137e-04\n",
      "\n",
      "Epoch 130: LearningRateScheduler setting learning rate to 0.0006113657011000002.\n",
      "Epoch 130/150\n",
      "528/528 [==============================] - 10s 20ms/step - loss: 0.4507 - accuracy: 0.7894 - val_loss: 0.6547 - val_accuracy: 0.6949 - lr: 6.1137e-04\n",
      "\n",
      "Epoch 131: LearningRateScheduler setting learning rate to 0.0006113657011000002.\n",
      "Epoch 131/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4498 - accuracy: 0.7895 - val_loss: 0.6492 - val_accuracy: 0.7005 - lr: 6.1137e-04\n",
      "\n",
      "Epoch 132: LearningRateScheduler setting learning rate to 0.0006113657011000002.\n",
      "Epoch 132/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4478 - accuracy: 0.7908 - val_loss: 0.6539 - val_accuracy: 0.6941 - lr: 6.1137e-04\n",
      "\n",
      "Epoch 133: LearningRateScheduler setting learning rate to 0.0006113657011000002.\n",
      "Epoch 133/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4469 - accuracy: 0.7915 - val_loss: 0.6675 - val_accuracy: 0.6833 - lr: 6.1137e-04\n",
      "\n",
      "Epoch 134: LearningRateScheduler setting learning rate to 0.0006113657011000002.\n",
      "Epoch 134/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4455 - accuracy: 0.7919 - val_loss: 0.6604 - val_accuracy: 0.6919 - lr: 6.1137e-04\n",
      "\n",
      "Epoch 135: LearningRateScheduler setting learning rate to 0.0006113657011000002.\n",
      "Epoch 135/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4444 - accuracy: 0.7923 - val_loss: 0.6784 - val_accuracy: 0.6943 - lr: 6.1137e-04\n",
      "\n",
      "Epoch 136: LearningRateScheduler setting learning rate to 0.0006113657011000002.\n",
      "Epoch 136/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4429 - accuracy: 0.7932 - val_loss: 0.6683 - val_accuracy: 0.6924 - lr: 6.1137e-04\n",
      "\n",
      "Epoch 137: LearningRateScheduler setting learning rate to 0.0006113657011000002.\n",
      "Epoch 137/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4419 - accuracy: 0.7940 - val_loss: 0.6718 - val_accuracy: 0.6963 - lr: 6.1137e-04\n",
      "\n",
      "Epoch 138: LearningRateScheduler setting learning rate to 0.0006113657011000002.\n",
      "Epoch 138/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4390 - accuracy: 0.7953 - val_loss: 0.6729 - val_accuracy: 0.6873 - lr: 6.1137e-04\n",
      "\n",
      "Epoch 139: LearningRateScheduler setting learning rate to 0.0006113657011000002.\n",
      "Epoch 139/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4397 - accuracy: 0.7952 - val_loss: 0.6706 - val_accuracy: 0.6992 - lr: 6.1137e-04\n",
      "\n",
      "Epoch 140: LearningRateScheduler setting learning rate to 0.0006113657011000002.\n",
      "Epoch 140/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4378 - accuracy: 0.7952 - val_loss: 0.6881 - val_accuracy: 0.6860 - lr: 6.1137e-04\n",
      "\n",
      "Epoch 141: LearningRateScheduler setting learning rate to 0.0006725022712100004.\n",
      "Epoch 141/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4409 - accuracy: 0.7943 - val_loss: 0.6670 - val_accuracy: 0.6969 - lr: 6.7250e-04\n",
      "\n",
      "Epoch 142: LearningRateScheduler setting learning rate to 0.0006725022712100004.\n",
      "Epoch 142/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4395 - accuracy: 0.7952 - val_loss: 0.6728 - val_accuracy: 0.6871 - lr: 6.7250e-04\n",
      "\n",
      "Epoch 143: LearningRateScheduler setting learning rate to 0.0006725022712100004.\n",
      "Epoch 143/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4387 - accuracy: 0.7950 - val_loss: 0.6995 - val_accuracy: 0.6905 - lr: 6.7250e-04\n",
      "\n",
      "Epoch 144: LearningRateScheduler setting learning rate to 0.0006725022712100004.\n",
      "Epoch 144/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4368 - accuracy: 0.7969 - val_loss: 0.6662 - val_accuracy: 0.6956 - lr: 6.7250e-04\n",
      "\n",
      "Epoch 145: LearningRateScheduler setting learning rate to 0.0006725022712100004.\n",
      "Epoch 145/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4365 - accuracy: 0.7971 - val_loss: 0.6823 - val_accuracy: 0.6862 - lr: 6.7250e-04\n",
      "\n",
      "Epoch 146: LearningRateScheduler setting learning rate to 0.0006725022712100004.\n",
      "Epoch 146/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4344 - accuracy: 0.7979 - val_loss: 0.6874 - val_accuracy: 0.6931 - lr: 6.7250e-04\n",
      "\n",
      "Epoch 147: LearningRateScheduler setting learning rate to 0.0006725022712100004.\n",
      "Epoch 147/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4336 - accuracy: 0.7984 - val_loss: 0.6835 - val_accuracy: 0.6909 - lr: 6.7250e-04\n",
      "\n",
      "Epoch 148: LearningRateScheduler setting learning rate to 0.0006725022712100004.\n",
      "Epoch 148/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4326 - accuracy: 0.7984 - val_loss: 0.6820 - val_accuracy: 0.6856 - lr: 6.7250e-04\n",
      "\n",
      "Epoch 149: LearningRateScheduler setting learning rate to 0.0006725022712100004.\n",
      "Epoch 149/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4320 - accuracy: 0.7989 - val_loss: 0.6858 - val_accuracy: 0.6836 - lr: 6.7250e-04\n",
      "\n",
      "Epoch 150: LearningRateScheduler setting learning rate to 0.0006725022712100004.\n",
      "Epoch 150/150\n",
      "528/528 [==============================] - 10s 19ms/step - loss: 0.4302 - accuracy: 0.7999 - val_loss: 0.6856 - val_accuracy: 0.6894 - lr: 6.7250e-04\n"
     ]
    }
   ],
   "source": [
    "modeltt = tf.keras.Sequential([\n",
    "  Conv2D(30, 2, padding='valid', activation='relu'),\n",
    "  MaxPool2D(),\n",
    "  Conv2D(60, 2, padding='valid', activation='relu'),\n",
    "  Conv2D(90, 2, padding='valid', activation='relu'),\n",
    "  Conv2D(60, 2, padding='valid', activation='relu'),\n",
    "  Conv2D(30, 2, padding='valid', activation='relu'),\n",
    "  MaxPool2D(),\n",
    "  Flatten(),\n",
    "  Dense(50, activation='relu'),\n",
    "  Dense(25, activation='relu'),\n",
    "  Dense(1, activation = \"sigmoid\")\n",
    "])\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0006995)\n",
    "  \n",
    "modeltt.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "initial_learning_rate = 0.0003451\n",
    "def lr_step_decay(epoch, lr):\n",
    "    drop_rate = 1.1\n",
    "    epochs_drop = 20\n",
    "    return initial_learning_rate * math.pow(drop_rate, math.floor(epoch/epochs_drop))\n",
    "\n",
    "historytt = modeltt.fit(datagen.flow(train_matrix, train_labels, batch_size=512,shuffle = True,seed=42), \n",
    "                        epochs=150, \n",
    "                        verbose = 1,\n",
    "                        callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_step_decay, verbose=1)],\n",
    "                        validation_data=datagen.flow(test_matrix, test_labels, batch_size=512,shuffle=True,seed=42))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8dde2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_results(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate model's accuracy, precision, recall and f1 score of a binary classification model.\n",
    "    \"\"\"\n",
    "    # Calculate accuracy\n",
    "    model_accuracy = accuracy_score(y_true, y_pred)*100\n",
    "    # Calculate model precision, recall and f1-score using \"weightd average\"\n",
    "    model_precision, model_recall, model_f1, __ = precision_recall_fscore_support(y_true, y_pred, average = \"weighted\")    # weighted average is good in the case when there is label imbalance\n",
    "    model_results = {\"accuracy\": model_accuracy,\n",
    "                     \"precision\": model_precision,\n",
    "                     \"recall\": model_recall,\n",
    "                     \"f1\": model_f1}\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "751e4463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "528/528 [==============================] - 7s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "predstt = modeltt.predict(datagen.flow(train_matrix, train_labels, batch_size=512, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a105f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "predstt = np.array(tf.round(predstt), dtype = 'int').reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9dabdd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 81.06333333333333,\n",
       " 'precision': 0.8107047848246909,\n",
       " 'recall': 0.8106333333333333,\n",
       " 'f1': 0.8106250612025592}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultstt = calculate_results(y_true=train_labels,y_pred=predstt)\n",
    "resultstt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f21b3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8106435284707729"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "restt = roc_auc_score(train_labels,predstt)\n",
    "restt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea6e9b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: commontask1_tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: commontask1_tf\\assets\n"
     ]
    }
   ],
   "source": [
    "modeltt.save(\"commontask1_tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0436eee0",
   "metadata": {},
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d1cd7d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported - ready to use PyTorch 2.0.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"Libraries imported - ready to use PyTorch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "43c88ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    print(\"Epoch:\", epoch)\n",
    "    # Process the images in batches\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Use the CPU or GPU as appropriate\n",
    "        # Recall that GPU is optimized for the operations we are dealing with\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Reset the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Push the data forward through the model layers\n",
    "        output = model(data)\n",
    "        \n",
    "        # Get the loss\n",
    "        loss = loss_criteria(output, target)\n",
    "\n",
    "        # Keep a running total\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print metrics so we see some progress\n",
    "        print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
    "            \n",
    "    # return average loss for the epoch\n",
    "    avg_loss = train_loss / (batch_idx+1)\n",
    "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b9b5c686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        for data, target in test_loader:\n",
    "            batch_count += 1\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Get the predicted classes for this batch\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate the loss for this batch\n",
    "            test_loss += loss_criteria(output, target).item()\n",
    "            \n",
    "            # Calculate the accuracy for this batch\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += torch.sum(target==predicted).item()\n",
    "\n",
    "    # Calculate the average loss and total accuracy for this epoch\n",
    "    avg_loss = test_loss / batch_count\n",
    "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    # return average loss for the epoch\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a351f923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e823cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(np.moveaxis(train_matrix[:40000],-1,1), dtype=torch.float32)\n",
    "y = torch.tensor(train_labels[:40000], dtype=torch.float32).reshape(-1, 1)\n",
    "xt = torch.tensor(np.moveaxis(test_matrix[:500],-1,1), dtype=torch.float32)\n",
    "yt = torch.tensor(test_labels[:500], dtype=torch.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "09cb0923",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    " \n",
    "def model_train(model, X_train, y_train, X_val, y_val):\n",
    "    # loss function and optimizer\n",
    "    loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005891)\n",
    " \n",
    "    n_epochs = 30   # number of epochs to run\n",
    "    batch_size = 200  # size of each batch\n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
    " \n",
    "    # Hold the best model\n",
    "    best_acc = - np.inf   # init to negative infinity\n",
    "    best_weights = None\n",
    " \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "                # take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "                # forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "                # print progress\n",
    "                acc = (y_pred.round() == y_batch).float().mean()\n",
    "                bar.set_postfix(\n",
    "                    loss=float(loss),\n",
    "                    acc=float(acc)\n",
    "                )\n",
    "        # evaluate accuracy at end of each epoch\n",
    "        model.eval()\n",
    "        y_pred = model(X_val)\n",
    "        acc = (y_pred.round() == y_val).float().mean()\n",
    "        acc = float(acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "    # restore model and return best accuracy\n",
    "    model.load_state_dict(best_weights)\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9d9ebf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(2, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(50, 75, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(75, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (drop): Dropout2d(p=0.2, inplace=False)\n",
      "  (fc1): Linear(in_features=25600, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (fc4): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## KAGGLE \n",
    "\n",
    "# Create a neural net class\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    \n",
    "    # Defining the Constructor\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=2, out_channels=50, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=50, out_channels=75, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=75, out_channels=100, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "        self.fc1 = nn.Linear(in_features=16* 16 * 100, out_features=200)\n",
    "        self.fc2 = nn.Linear(in_features=200, out_features=100) \n",
    "        self.fc3 = nn.Linear(in_features=100, out_features=50)\n",
    "        self.fc4 = nn.Linear(in_features=50, out_features=1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # In the forward function, pass the data through the layers we defined in the init function\n",
    "        # Use a ReLU activation function after layer 1 (convolution 1 and pool)\n",
    "        x = F.relu(self.conv1(x)) \n",
    "        x = F.relu(self.conv2(x)) \n",
    "        x = F.relu(self.pool(self.conv3(x)))  \n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "        # Feed to fully-connected layer to predict class\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))      \n",
    "        x=self.fc4(x)\n",
    "        \n",
    "        # Return class probabilities via a sigmoid function \n",
    "        return F.sigmoid(x)\n",
    "    \n",
    "device = \"cpu\"\n",
    "if (torch.cuda.is_available()):\n",
    "    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n",
    "    device = \"cuda\"\n",
    "\n",
    "# Create an instance of the model class and allocate it to the device\n",
    "model = Net(num_classes=2).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dc6e2dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a842b75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6240000128746033"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model_train(model,x,y,xt,yt)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4e82f307",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'commontask1_torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160c7cd5",
   "metadata": {},
   "source": [
    "# Specific Task: Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0f2a2294",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 32           # We'll resize input images to this size\n",
    "patch_size = 2             # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]                              # Size of the transformer layers\n",
    "transformer_layers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dd14dd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(tf.keras.layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = images.shape[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1,self.patch_size, self.patch_size,1],\n",
    "            strides=[1,self.patch_size, self.patch_size,1],\n",
    "            rates=[1,1,1,1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "364bdec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = tf.keras.layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "44667cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.lay_norm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.fc1 = tf.keras.layers.Dense(1280,activation=\"relu\")\n",
    "        self.fc2 = tf.keras.layers.Dense(64,activation=\"relu\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "33cdb096",
   "metadata": {},
   "outputs": [],
   "source": [
    "class vit(BaseAttention):\n",
    "    def call(self,inputs):\n",
    "        inputs = tf.expand_dims(tf.squeeze(inputs,axis=0),axis=0)\n",
    "        # Create patches.\n",
    "        patches = Patches(patch_size)(inputs)\n",
    "        # Encode patches.\n",
    "        encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "        for _ in range(12):\n",
    "            lay_normed = self.lay_norm(encoded_patches)\n",
    "            msar = self.mha(lay_normed,lay_normed)\n",
    "            added = self.add([msar,encoded_patches])\n",
    "            lay_normed2 = self.lay_norm(added)\n",
    "            act = tf.keras.activations.gelu(lay_normed2)\n",
    "            fc1 = self.fc1(act)\n",
    "            fc2 = self.fc2(fc1)\n",
    "            encoded_patches = self.add([fc2,added])\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3c066c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = vit(num_heads=4, key_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d3705d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer():\n",
    "    def layer(self,inputs):\n",
    "        x = tf.keras.layers.LayerNormalization()(inputs)\n",
    "        x = tf.keras.activations.tanh(x)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d301f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3725557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():    \n",
    "    inputs = tf.keras.Input(shape = (32,32,2))\n",
    "    print(inputs.shape)\n",
    "    x = vit.call(inputs)\n",
    "    x = layer.layer(x)\n",
    "    print(x.shape)\n",
    "    outputs = tf.keras.layers.Dense(units=1, activation='sigmoid')(x)\n",
    "    print(outputs.shape)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=\n",
    "                                                    0.007), \n",
    "                   loss= \"binary_crossentropy\", \n",
    "                   metrics=[\"accuracy\"])\n",
    "    global lr_scheduler\n",
    "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4*10**(epoch/20))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "554ce8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 32, 32, 2)\n",
      "(1, 2048)\n",
      "(1, 1)\n",
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, 32, 32, 2)]       0         \n",
      "                                                                 \n",
      " tf.compat.v1.squeeze_15 (TF  (32, 32, 2)              0         \n",
      " OpLambda)                                                       \n",
      "                                                                 \n",
      " tf.expand_dims_15 (TFOpLamb  (1, 32, 32, 2)           0         \n",
      " da)                                                             \n",
      "                                                                 \n",
      " layer_normalization_18 (Lay  (1, 32, 32, 2)           4         \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " tf.math.tanh_13 (TFOpLambda  (1, 32, 32, 2)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten_15 (Flatten)        (1, 2048)                 0         \n",
      "                                                                 \n",
      " dense_50 (Dense)            (1, 1)                    2049      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,053\n",
      "Trainable params: 2,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_vit = build_model()\n",
    "model_vit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0ed95d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array(train_matrix)\n",
    "valid_data = np.array(test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "613c2491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((270000, 32, 32, 2), (270000, 1))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_data).shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b225f402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 3s 2ms/step - loss: 0.7074 - accuracy: 0.5450 - val_loss: 0.7192 - val_accuracy: 0.5300\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.6441 - accuracy: 0.6330 - val_loss: 0.7803 - val_accuracy: 0.5100\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5772 - accuracy: 0.6860 - val_loss: 0.9927 - val_accuracy: 0.5200\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5545 - accuracy: 0.7160 - val_loss: 1.0296 - val_accuracy: 0.4900\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5179 - accuracy: 0.7410 - val_loss: 0.9593 - val_accuracy: 0.4800\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4875 - accuracy: 0.7630 - val_loss: 1.2163 - val_accuracy: 0.5200\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4778 - accuracy: 0.7690 - val_loss: 1.1372 - val_accuracy: 0.4900\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4604 - accuracy: 0.7730 - val_loss: 1.2629 - val_accuracy: 0.4800\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4549 - accuracy: 0.7830 - val_loss: 1.1506 - val_accuracy: 0.5000\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4198 - accuracy: 0.7930 - val_loss: 1.3633 - val_accuracy: 0.4800\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4116 - accuracy: 0.8110 - val_loss: 1.3420 - val_accuracy: 0.4900\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.4107 - accuracy: 0.8030 - val_loss: 1.5151 - val_accuracy: 0.4900\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3898 - accuracy: 0.8200 - val_loss: 1.5197 - val_accuracy: 0.4800\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3858 - accuracy: 0.8270 - val_loss: 1.5126 - val_accuracy: 0.5200\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3827 - accuracy: 0.8180 - val_loss: 1.6831 - val_accuracy: 0.5000\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3710 - accuracy: 0.8250 - val_loss: 1.5752 - val_accuracy: 0.4900\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3511 - accuracy: 0.8400 - val_loss: 1.6369 - val_accuracy: 0.5000\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3557 - accuracy: 0.8430 - val_loss: 1.7755 - val_accuracy: 0.4900\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3460 - accuracy: 0.8350 - val_loss: 1.9297 - val_accuracy: 0.5200\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3386 - accuracy: 0.8400 - val_loss: 1.8900 - val_accuracy: 0.5000\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3236 - accuracy: 0.8510 - val_loss: 1.9980 - val_accuracy: 0.4900\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3276 - accuracy: 0.8460 - val_loss: 2.0168 - val_accuracy: 0.5200\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3147 - accuracy: 0.8600 - val_loss: 2.0550 - val_accuracy: 0.5600\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3130 - accuracy: 0.8580 - val_loss: 2.1291 - val_accuracy: 0.5100\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2984 - accuracy: 0.8650 - val_loss: 2.1546 - val_accuracy: 0.5200\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2896 - accuracy: 0.8560 - val_loss: 2.2067 - val_accuracy: 0.5100\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2985 - accuracy: 0.8690 - val_loss: 2.3750 - val_accuracy: 0.5200\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2721 - accuracy: 0.8880 - val_loss: 2.3965 - val_accuracy: 0.5100\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2683 - accuracy: 0.8800 - val_loss: 2.3980 - val_accuracy: 0.5100\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2936 - accuracy: 0.8690 - val_loss: 2.5080 - val_accuracy: 0.5100\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2462 - accuracy: 0.8940 - val_loss: 2.5355 - val_accuracy: 0.5100\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2413 - accuracy: 0.8910 - val_loss: 2.6649 - val_accuracy: 0.5000\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2374 - accuracy: 0.8940 - val_loss: 2.6848 - val_accuracy: 0.4800\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2414 - accuracy: 0.8950 - val_loss: 2.7224 - val_accuracy: 0.5300\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2477 - accuracy: 0.9030 - val_loss: 2.7205 - val_accuracy: 0.4600\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2252 - accuracy: 0.8990 - val_loss: 2.8016 - val_accuracy: 0.4900\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2226 - accuracy: 0.9050 - val_loss: 2.7801 - val_accuracy: 0.5000\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2194 - accuracy: 0.8960 - val_loss: 2.8297 - val_accuracy: 0.4900\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2119 - accuracy: 0.9140 - val_loss: 2.9006 - val_accuracy: 0.5000\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2177 - accuracy: 0.9050 - val_loss: 2.8873 - val_accuracy: 0.5100\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1960 - accuracy: 0.9130 - val_loss: 3.0109 - val_accuracy: 0.5200\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2038 - accuracy: 0.9120 - val_loss: 2.9263 - val_accuracy: 0.5300\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1940 - accuracy: 0.9220 - val_loss: 3.0263 - val_accuracy: 0.4800\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1848 - accuracy: 0.9270 - val_loss: 3.1605 - val_accuracy: 0.4900\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9250 - val_loss: 3.1906 - val_accuracy: 0.4800\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9190 - val_loss: 3.2243 - val_accuracy: 0.4900\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9240 - val_loss: 3.2018 - val_accuracy: 0.5100\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1781 - accuracy: 0.9250 - val_loss: 3.2947 - val_accuracy: 0.5100\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1794 - accuracy: 0.9280 - val_loss: 3.1599 - val_accuracy: 0.4800\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1585 - accuracy: 0.9390 - val_loss: 3.2925 - val_accuracy: 0.4700\n"
     ]
    }
   ],
   "source": [
    "history_vit = model_vit.fit(datagen.flow(train_data[:1000], train_labels[:1000], batch_size=1,shuffle = False), \n",
    "                    epochs=50, \n",
    "                    verbose = 1,\n",
    "#                     callbacks=[lr_scheduler],\n",
    "                   validation_data=datagen.flow(valid_data[:100], test_labels[:100], batch_size=1,shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9703c8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 145/1000 [===>..........................] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\preprocessing\\image.py:766: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (1000, 32, 32, 2) (2 channels).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 869us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 94.89999999999999,\n",
       " 'precision': 0.9522253619396477,\n",
       " 'recall': 0.949,\n",
       " 'f1': 0.9488345945945946}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_vit = model_vit.predict(datagen.flow(train_matrix[:1000], train_labels[:1000], batch_size=1, shuffle=False))\n",
    "preds_vit = np.array(tf.round(preds_vit), dtype = 'int').reshape(-1,)\n",
    "results_vit = calculate_results(y_true=train_labels[:1000],y_pred=preds_vit)\n",
    "results_vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b0bb1d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9475703119994875"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_vit = roc_auc_score(train_labels[:1000],preds_vit)\n",
    "res_vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e49ea412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vit\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vit\\assets\n"
     ]
    }
   ],
   "source": [
    "model_vit.save('vit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5604b77e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960760ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8671a835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
